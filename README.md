# To Grant or Not to Grant: Deciding on Compensation Benefits

## ðŸŽ¯ Project Vision: Revolutionizing Workers' Compensation through Automation

In the rapidly evolving world of data science, Machine Learning (ML) is making significant strides in transforming traditional processes. One of the most impactful applications is in automating decision-making in bureaucratic systems. This project focuses on automating the decision process for workers' compensation claims at the New York Workers' Compensation Board (WCB). Our goal was to develop an ML model capable of assessing claims, reducing the burden on human workers and increasing efficiency in compensating the deserving employees.

## ðŸ’¡ Why This Project Matters

Workers' compensation claims are an essential part of supporting employees who get injured or ill while working. However, the traditional claim adjudication process at WCB is entirely manual, involving lengthy reviews that can lead to delays, resource exhaustion, and ultimately frustration for employees who need timely support. 

In this context, the aim was clear: design and implement a system that could reduce human effort, make decisions faster, and ensure fairness across the board. This project represents a significant step towards modernizing the claims process through automation.

## ðŸ‘¥ The Team: A Blend of Skills and Expertise

This project was brought to life by a dedicated team of data science and machine learning enthusiasts, all students in the Master in Data Science and Advanced Analytics program at NOVA Information Management School:

- **Eldar Medvedev** (r20181162)
- **Guilherme Godinho** (20211552)
- **JÃ©ssica Cristas** (20240488)
- **Joshua Wehr** (20240501)
- **Umeima Mahomed** (20240543)

Each member contributed their unique skills to the project, from data exploration to model evaluation and report writing.

## ðŸ“Š Project Deliverables

Our journey was not just about building a model; it was about creating a comprehensive solution that could be utilized, evaluated, and enhanced over time. Our **final** deliverables include:

**Machine Learning Model:** The classification model, developed to predict whether a claim should be granted based on the injury type, is encapsulated in the file `XGB_Optuna.pkl`, which is essential for advanced evaluations and application in the open-ended section.

**Jupyter Notebooks:** The project includes three key notebooks:
- `ML_Project_Group50_DataExp&Prep.ipynb` covers the initial phases of data exploration and preprocessing.
- `ML_Project_Group50_AgreementReached.ipynb` demonstrates a standalone prediction workflow.
- `ML_Project_Group50_Exodia.ipynb` consolidates preprocessing, modeling, evaluation, and insights from the open-ended section.
- Web Application: The file `WebApp.py` enables further exploration and analysis, supported by documentation in WebApp_README.txt.
  
**Project Report:** The methodology, challenges, and outcomes are embedded throughout the files, with the open-ended section providing additional insights for future improvements.

## ðŸ”„ Our Approach: A Data Science Journey with CRISP-DM

We adopted the CRISP-DM methodology (Cross-Industry Standard Process for Data Mining) to ensure a structured, step-by-step approach. Here's how we used it:

1. **Business Understanding**: Understanding the WCB's pain points in handling compensation claims, setting the project's objective to automate the decision-making process.
2. **Data Understanding**: We explored the dataset, understood its structure, and identified areas for cleaning and preprocessing.
3. **Data Preparation**: Using Python tools like pandas and NumPy, we cleaned and prepared the data, ensuring it was ready for model building.
4. **Modelling**: We experimented with various machine learning algorithms using scikit-learn to train the model to classify claims accurately.
5. **Evaluation**: The model's performance was evaluated using accuracy, precision, and recall metrics to determine its effectiveness.
6. **Deployment**: While the focus of this project was more theoretical, we outlined how the model could be implemented in a real-world system.

## ðŸ›  Technologies Behind the Model

Our work leveraged the power of Python 3.10, with libraries that allowed us to efficiently clean, analyze, and model the data:

- **pandas**: For data manipulation and analysis.
- **scikit-learn**: To build, train, and evaluate the machine learning model.
- **matplotlib, leaflet**: For visualizations to help in data exploration and model evaluation.
- **Jupyter Notebook**: For creating and sharing our live code, visualizations, and results in an interactive manner.

## ðŸš€ Getting Started

To get started with our project, simply clone the repository and run the Jupyter Notebooks to explore the data, model, and the full implementation.
